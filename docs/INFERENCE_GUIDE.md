# Inference Guide

This guide covers running inference with CoreML language models converted using `lm_conversion_example.py`.

## Overview

The inference script provides:
- **Interactive chat interface** with KV cache management
- **Single/chunked model support**
- **Top-k and top-p (nucleus) sampling**
- **Real-time token streaming**
- **Timing statistics**

## Quick Start

```bash
# Single model
uv run python examples/inference.py \
    --model-dir ./qwen2_0.5b_seqlen_8.mlpackage \
    --model-name Qwen/Qwen2-0.5B

# Chunked model
uv run python examples/inference.py \
    --model-dir ./qwen3_4b_chunked_4 \
    --model-name Qwen/Qwen3-4B \
    --chunked --num-chunks 4
```

## Required Files

The inference script requires:

| File | Description | Generated By |
|------|-------------|--------------|
| `*.mlpackage` or `chunk_*.mlpackage` | CoreML model(s) | `--num-chunks` |
| `embeddings.npy` | Token embeddings (float16) | `--export-embeddings` |
| `lm_head.mlpackage` | Language model head | `--export-lm-head` |

Example directory structure for chunked model:
```
qwen3_4b_chunked_4/
├── chunk_0.mlpackage
├── chunk_1.mlpackage
├── chunk_2.mlpackage
├── chunk_3.mlpackage
├── embeddings.npy
└── lm_head.mlpackage
```

## CLI Arguments Reference

| Argument | Default | Description |
|----------|---------|-------------|
| `--model-dir` | required | Path to model .mlpackage or chunked directory |
| `--model-name` | required | HuggingFace model name (for tokenizer) |
| `--embeddings` | auto | Path to embeddings.npy |
| `--lm-head` | auto | Path to lm_head.mlpackage |
| `--chunked` | false | Use chunked model |
| `--num-chunks` | 1 | Number of model chunks |
| `--max-context-length` | auto | Maximum KV cache size |
| `--cache-compiled` | false | Cache compiled models for faster loads |
| `--temperature` | 0.8 | Sampling temperature |
| `--top-k` | 40 | Top-k filtering (0 = disabled) |
| `--top-p` | 0.95 | Nucleus sampling threshold |
| `--max-new-tokens` | 100 | Maximum tokens per response |
| `--system-prompt` | none | System prompt for chat |
| `--prompt` | none | Single prompt (non-interactive mode) |
| `--quiet` | false | Reduce output verbosity |

## Inference Modes

### Interactive Chat Mode

Default mode with persistent KV cache across turns:

```bash
uv run python examples/inference.py \
    --model-dir ./model \
    --model-name Qwen/Qwen2-0.5B \
    --system-prompt "You are a helpful assistant."
```

**Chat commands**:
- `/reset` - Clear conversation history and KV cache
- `/quit` - Exit chat
- `/help` - Show available commands

### Non-Interactive Mode

Single prompt, single response:

```bash
uv run python examples/inference.py \
    --model-dir ./model \
    --model-name Qwen/Qwen2-0.5B \
    --prompt "What is machine learning?"
```

## Sampling Parameters

### Temperature

Controls randomness (higher = more random):

```bash
--temperature 0.1  # More deterministic
--temperature 0.8  # Balanced (default)
--temperature 1.5  # More creative
```

### Top-k Filtering

Only sample from top k most likely tokens:

```bash
--top-k 0   # Disabled
--top-k 40  # Default - consider top 40 tokens
--top-k 10  # More focused
```

### Top-p (Nucleus) Sampling

Sample from smallest set of tokens with cumulative probability >= p:

```bash
--top-p 1.0   # Disabled
--top-p 0.95  # Default - 95% probability mass
--top-p 0.8   # More focused
```

### Combined Example

```bash
uv run python examples/inference.py \
    --model-dir ./model \
    --model-name Qwen/Qwen2-0.5B \
    --temperature 0.7 \
    --top-k 50 \
    --top-p 0.9 \
    --max-new-tokens 200
```

## KV Cache Management

### Context Length

The KV cache has a fixed maximum length (typically 2048 tokens). The script:
- Tracks current position in the cache
- Displays usage statistics after each generation
- Auto-resets when cache fills up

### Cache Status Indicators

After generation, the script shows:
```
KV Cache usage:
  Position: 512 / 2048 (25.0%) [OK]
  Remaining capacity: 1536 tokens
```

Status levels:
- `[OK]` - Under 75% usage
- `[WARNING]` - 75-90% usage
- `[CRITICAL]` - Over 90% usage

### Manual Reset

Use `/reset` command to clear the cache and start a new conversation.

## Architecture

### Token Processing Flow

```
1. Tokenize input text
2. Embed tokens: token_ids → embeddings.npy → (1, hidden_dim, 1, seq_len)
3. Process through model chunks (updates KV cache)
4. Compute logits via lm_head.mlpackage
5. Sample next token
6. Repeat from step 2 for generation
```

### Chunked Model Inference

For chunked models, hidden states flow through all chunks sequentially:

```
Input → Chunk 0 → Chunk 1 → ... → Chunk N → Output
           ↓           ↓              ↓
        KV Cache   KV Cache       KV Cache
```

Each chunk maintains its own KV cache state.

### Prompt Chunking

Long prompts are processed in chunks matching the model's `seq_len`:

```
Prompt: 100 tokens, seq_len: 8
→ Process tokens 0-7 (chunk 1)
→ Process tokens 8-15 (chunk 2)
→ ...
→ Process tokens 96-99 + padding (chunk 13)
```

Padding uses the last token value (doesn't affect output due to causal masking).

## LM Head Output

The LM head returns:
- `logits`: Shape `(1, vocab_size, 1, seq_len)`
- `chunk_logsumexp_stable`: For stable softmax computation
- `chunk_max`: Per-chunk maximum values

The inference script combines these for numerically stable probability computation.

## Timing Statistics

After each generation, the script displays:

```
TIMING STATISTICS
--------------------------------------------------
Prompt processing:
  Tokens: 45
  Model forward time: 150.23 ms
  Speed: 299.54 tokens/sec

Token generation:
  Tokens generated: 50
  Model forward time: 1250.45 ms (25.01 ms/token)
  LM head time: 125.30 ms (2.51 ms/token)
  Sampling time: 5.20 ms (0.10 ms/token)
  Total generation time: 1380.95 ms
  Speed: 36.21 tokens/sec
```

## Programmatic Usage

```python
from examples.inference import CoreMLLanguageModel, generate, chat_loop
from transformers import AutoTokenizer

# Load model
model = CoreMLLanguageModel(
    model_dir="./qwen3_4b_chunked_4",
    embeddings_path="./qwen3_4b_chunked_4/embeddings.npy",
    lm_head_path="./qwen3_4b_chunked_4/lm_head.mlpackage",
    is_chunked=True,
    num_chunks=4,
)

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-4B")

# Generate text
response = generate(
    model=model,
    tokenizer=tokenizer,
    prompt="What is AI?",
    max_new_tokens=100,
    temperature=0.8,
    top_k=40,
    top_p=0.95,
)
```

## Common Issues

### Out of Memory During Inference

- Reduce `--max-context-length` if auto-detection picks too large
- Use `/reset` to clear the KV cache

### Slow Generation

- Verify Neural Engine is being used (check conversion with `--analyze-compute-plan`)
- Lower `--max-new-tokens` if responses are too long
- Use chunked model if single model is too large for Neural Engine

### Poor Quality Output

- Adjust sampling parameters:
  - Lower temperature for more focused output
  - Increase top-k for more diversity
  - Adjust top-p for better probability distribution
- Add a system prompt to guide behavior
- Check if the model was converted correctly (verification should pass)

### KV Cache Overflow

When the cache fills up:
- In chat mode: automatic reset with notification
- In single prompt mode: generation stops

Use `/reset` proactively when context is getting full.

## Compiled Model Caching

CoreML compiles `.mlpackage` models to optimized `.mlmodelc` format on first load. This compilation can take significant time for large models. The `--cache-compiled` flag saves these compiled models for faster subsequent loads.

### How It Works

1. **First run with `--cache-compiled`**: Models are loaded from `.mlpackage`, compiled, and the compiled `.mlmodelc` is saved alongside
2. **Subsequent runs**: Compiled models are detected and loaded directly (much faster)

### Usage

```bash
# First run: compiles and caches
uv run python examples/inference.py \
    --model-dir ./qwen3_4b_chunked_4 \
    --model-name Qwen/Qwen3-4B \
    --chunked --num-chunks 4 \
    --cache-compiled

# Subsequent runs: loads cached compiled models (faster)
uv run python examples/inference.py \
    --model-dir ./qwen3_4b_chunked_4 \
    --model-name Qwen/Qwen3-4B \
    --chunked --num-chunks 4
```

### Cached File Structure

After caching, your model directory will contain:

```
qwen3_4b_chunked_4/
├── chunk_0.mlpackage        # Original model
├── chunk_0.mlmodelc/        # Cached compiled model
├── chunk_1.mlpackage
├── chunk_1.mlmodelc/
├── ...
├── lm_head.mlpackage
├── lm_head.mlmodelc/
└── embeddings.npy
```

### Automatic Detection

The inference script automatically detects cached compiled models. If a `.mlmodelc` exists alongside a `.mlpackage`, it will be used automatically (no `--cache-compiled` flag needed).

### Notes

- Compiled models are device-specific and may need regeneration on different hardware
- The `.mlmodelc` directories can be safely deleted to force recompilation
- Caching applies to the main model chunks and LM head
